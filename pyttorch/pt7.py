import torch

a = torch.FloatTensor([1, 2, 3, 4, 5, 6])
print(a.sum()) # tensor(21.) - сумма элементов тензора(вне зависимости от размерности)

ittem = a.sum().item()
print(ittem) # 21.0
print(type(ittem)) # <class 'float'> питоновский float

print(a.mean()) # среднее значение
print(a.max()) # макс значение tensor(6.)
print(a.min()) # мин. значение tensor(1.)

# a = a.reshape(3, 2)
# print(a)

a = a.view(3, 2) # тоже самое что и reshape
print(a)

sum0 = a.sum(dim=0) # сложатся элементы столбцов
print(sum0) # tensor([ 9., 12.]) то есть 1+3+5 и 2+4+6

sum0 = a.sum(dim=1) # сложатся элементы строк
print(sum0) # tensor([ 3.,  7., 11.])


# среднее значение по осям
dim0 = a.mean(dim=0) # среднее значение по столбцам(0 ось)
print(dim0) # tensor([3., 4.])

dim1 = a.mean(dim=1) # можно вторым параметром задать keepdim = True, тогда разобьется на 3х1 тензор
print(dim1) # tensor([1.5000, 3.5000, 5.5000])
dim11 = a.mean(dim=1, keepdim=True)
print(dim11, dim11.shape) # tensor([[1.5000], [3.5000], [5.5000]]) torch.Size([3, 1])

# аналогично для осей можно min и max искать:
max0 = a.max(dim=0).values
print(max0) # по столбцам - tensor([5., 6.])

max1 = a.max(dim=1).values
print(max1) # по строкам - tensor([2., 4., 6.])

# ГОРАЗДО УДОБНЕЕ ИСПОЛЬЗОВАТЬ ДЛЯ МНОГОМЕРНЫХ ТЕНЗОРОВ МЕТОД amax(dim=...)
max0 = a.amax(dim=0) # не надо ставить .values
max1 = a.amax(dim=1)
print(max0, '\n', max1) # tensor([5., 6.]) по столбцам tensor([2., 4., 6.]) по строкам

a = torch.IntTensor([1, 2, -3, -4, 5, 6]) # tensor([ 1,  2, -3, -4,  5,  6], dtype=torch.int32)
print(a)
b = a.abs() # tensor([1, 2, 3, 4, 5, 6], dtype=torch.int32)
print(b)
a.abs_() # изменит сам a и сделает все значения по модулю
print(a) # tensor([1, 2, 3, 4, 5, 6], dtype=torch.int32)

print(torch.amax(a))

logna = torch.log(a)
print(logna) # натуральный логарифм ко всем элементам a - tensor([0.0000, 0.6931, 1.0986, 1.3863, 1.6094, 1.7918])

print(torch.round(logna)) # округлили - tensor([0., 1., 1., 1., 2., 2.])

a = a.float() # сделали a вещественным
a.log_() # применили логарифм к самому тензору а
print(a) # tensor([0.0000, 0.6931, 1.0986, 1.3863, 1.6094, 1.7918])

a = torch.linspace(0, torch.pi, 10) # разделили на 10 интервалов от 0 до 3.14168 и подставили в a
print(a) # tensor([0.0000, 0.3491, 0.6981, 1.0472, 1.3963, 1.7453, 2.0944, 2.4435, 2.7925, 3.1416])

r = torch.sin(a)
print(r) # tensor([ 0.0000e+00,  3.4202e-01,  6.4279e-01,  8.6603e-01,  9.8481e-01, 9.8481e-01,  8.6603e-01,  6.4279e-01,  3.4202e-01, -8.7423e-08])

r = torch.cos(a)
print(r) # tensor([ 1.0000,  0.9397,  0.7660,  0.5000,  0.1736, -0.1736, -0.5000, -0.7660, -0.9397, -1.0000])

a.tan_() # тангенс применится К САМОМУ a
print(a) # tensor([ 0.0000e+00,  3.6397e-01,  8.3910e-01,  1.7321e+00,  5.6713e+00, -5.6713e+00, -1.7321e+00, -8.3910e-01, -3.6397e-01,  8.7423e-08])


x = torch.FloatTensor([1, 4, 3, 7, 10, 8, 14, 21, 20, 23])
y = torch.FloatTensor([4, 1, 6, 9, 13, 11, 16, 19, 15, 22])

medx = torch.median(x)
print(medx) # tensor(8.) - цифра 8(медиана, половина больше, половина - меньше)
# среднее арифметическое - 11.1, дисперсия = ((1-11.1)^2 + (4-11.1)^2 + ... + (20-11.1)^2 + (23 - 11.1)^2)/(10-1) = 572.9/9 = 63.6556
dispx = x.var() # дисперсия
print(dispx) # tensor(63.6556)
stdx = x.std()
print(stdx) # 7.9784 (стандарт отклонения = корень из дисперсии)
print(stdx ** 2, dispx) # tensor(63.6556) tensor(63.6556)

medy = torch.median(y)
print(medy) # tensor(11.)
stdy = y.std() # стандарт отклонения
print(stdy) # tensor(6.7032) - 6.7032


# объединяем 2 тензора:
xy = torch.vstack([x, y]) # объединяем в матрицу из векторо x и y
print(xy) # tensor([[ 1.,  4.,  3.,  7., 10.,  8., 14., 21., 20., 23.],  [ 4.,  1.,  6.,  9., 13., 11., 16., 19., 15., 22.]])


# НАХОДИМ КОРРЕЛЯЦИЮ:
corrxy = torch.corrcoef(xy) # корреляция значит [[x к самому себе, x к y], [y к x, y к самому себе]]
print(corrxy) # tensor([[1.0000, 0.9316], [0.9316, 1.0000]]) - матрица

# Значение 0.9316 говорит о сильной положительной линейной зависимости между x и y:
# когда x увеличивается, y тоже увеличивается, и наоборот.

# Диапазон r от −1 (идеальная отрицательная корреляция) до 1 (идеальная положительная корреляция). 0 — отсутствие линейной зависимости.

# автоковариационная матрица:
covxy = torch.cov(xy)
print(covxy) # tensor([[63.6556, 49.8222], [49.8222, 44.9333]]), корреляция более удобна для анализа
